{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1da34ff-9526-4cf9-80f3-8ea803afef58",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-16T01:25:18.253720Z",
     "iopub.status.busy": "2024-07-16T01:25:18.253373Z",
     "iopub.status.idle": "2024-07-16T01:25:20.058174Z",
     "shell.execute_reply": "2024-07-16T01:25:20.057641Z",
     "shell.execute_reply.started": "2024-07-16T01:25:18.253697Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [00:01, 120957.85it/s]\n",
      "10000it [00:00, 129748.16it/s]\n",
      "10000it [00:00, 57614.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from train_eval import train, init_network\n",
    "from importlib import import_module\n",
    "import argparse\n",
    "dataset = 'THUCNews'  # 数据集\n",
    "\n",
    "# 搜狗新闻:embedding_SougouNews.npz, 腾讯:embedding_Tencent.npz, 随机初始化:random\n",
    "embedding = 'embedding_SougouNews.npz'\n",
    "\n",
    "model_name = 'TextRCNN'  # 'TextRCNN'  # TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer\n",
    "\n",
    "from utils import build_dataset, build_iterator, get_time_dif\n",
    "\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config(dataset, embedding)\n",
    "vocab, train_data, dev_data, test_data = build_dataset(config, False)\n",
    "train_iter = build_iterator(train_data, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777ac001-3cbf-476a-8d4e-8e02ded46588",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-16T01:25:45.307431Z",
     "iopub.status.busy": "2024-07-16T01:25:45.307098Z",
     "iopub.status.idle": "2024-07-16T01:25:45.311560Z",
     "shell.execute_reply": "2024-07-16T01:25:45.311011Z",
     "shell.execute_reply.started": "2024-07-16T01:25:45.307411Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 4, 3, 5, 5, 1, 0, 2, 5, 8, 4, 6, 7, 5, 0, 0, 7, 8, 1, 7, 8, 8, 9,\n",
       "        1, 3, 0, 3, 5, 0, 1, 8, 6, 1, 7, 9, 2, 3, 4, 0, 7, 9, 5, 7, 9, 0, 1, 6,\n",
       "        3, 6, 7, 9, 3, 6, 1, 8, 6, 8, 1, 2, 9, 2, 7, 9, 9, 7, 7, 6, 8, 8, 6, 8,\n",
       "        7, 8, 3, 4, 3, 3, 2, 0, 0, 9, 6, 8, 6, 5, 4, 7, 6, 7, 8, 1, 8, 8, 5, 9,\n",
       "        9, 6, 9, 1, 5, 4, 9, 1, 3, 6, 7, 1, 0, 0, 8, 6, 5, 8, 1, 4, 0, 2, 0, 8,\n",
       "        5, 3, 3, 9, 9, 6, 4, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter.__next__()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1415aa-2351-40ef-a102-87c300fc5fa7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T06:27:58.789972Z",
     "iopub.status.busy": "2024-07-15T06:27:58.789223Z",
     "iopub.status.idle": "2024-07-15T06:27:58.912391Z",
     "shell.execute_reply": "2024-07-15T06:27:58.911192Z",
     "shell.execute_reply.started": "2024-07-15T06:27:58.789936Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = x.Config(dataset, embedding)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92608af9-a216-45dc-82d5-3f3580f42442",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T06:28:36.199629Z",
     "iopub.status.busy": "2024-07-15T06:28:36.198726Z",
     "iopub.status.idle": "2024-07-15T06:28:36.204537Z",
     "shell.execute_reply": "2024-07-15T06:28:36.203084Z",
     "shell.execute_reply.started": "2024-07-15T06:28:36.199580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.pad_size = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67208c6e-0ccc-4aa5-841a-b80aefe95d09",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T07:35:29.198657Z",
     "iopub.status.busy": "2024-07-15T07:35:29.197907Z",
     "iopub.status.idle": "2024-07-15T07:35:29.207143Z",
     "shell.execute_reply": "2024-07-15T07:35:29.205915Z",
     "shell.execute_reply.started": "2024-07-15T07:35:29.198608Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List elements have been saved to Newtrain/data/class.txt\n"
     ]
    }
   ],
   "source": [
    "# 假设你有一个列表\n",
    "my_list = ['价格', '配置', '操控', '舒适性', '油耗', '动力', '内饰', '安全性', '空间', '外观']\n",
    "\n",
    "# 指定文件名\n",
    "filename = 'Newtrain/data/class.txt'\n",
    "\n",
    "# 打开文件以写入模式 ('w' 表示写入，如果文件已存在则会被覆盖)\n",
    "with open(filename, 'w') as file:\n",
    "    # 遍历列表，将每个元素写入文件，每个元素后跟换行符('\\n')\n",
    "    for element in my_list:\n",
    "        file.write(element + '\\n')\n",
    "\n",
    "print(f\"List elements have been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6edb20-74ca-42a6-ad9f-fb9e08d5a291",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T02:47:48.000179Z",
     "iopub.status.busy": "2024-07-15T02:47:47.999788Z",
     "iopub.status.idle": "2024-07-15T02:47:49.844909Z",
     "shell.execute_reply": "2024-07-15T02:47:49.844323Z",
     "shell.execute_reply.started": "2024-07-15T02:47:48.000158Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocab size: 4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [00:01, 114935.30it/s]\n",
      "10000it [00:00, 131912.53it/s]\n",
      "10000it [00:00, 51791.44it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "vocab, train_data, dev_data, test_data = build_dataset(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f35130-50c4-46a4-9271-77e256e8f5d6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T02:48:21.020883Z",
     "iopub.status.busy": "2024-07-15T02:48:21.020454Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = np.random.rand(len(vocab), 300)\n",
    "pretrain_dir = \"THUCNews/data/sgns.sogou.char\"\n",
    "f = open(pretrain_dir, \"r\", encoding='UTF-8')\n",
    "for i, line in enumerate(f.readlines()):\n",
    "    \n",
    "    # if i == 0:  # 若第一行是标题，则跳过\n",
    "    #     continue\n",
    "    lin = line.strip().split(\" \")\n",
    "    print(lin)\n",
    "    if lin[0] in vocab:\n",
    "        idx = vocab[lin[0]]\n",
    "        emb = [float(x) for x in lin[1:301]]\n",
    "        embeddings[idx] = np.asarray(emb, dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded28839-9cb3-4a29-a4bb-f03bebfc1024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0063007-d015-4a83-b9c2-3505958cfc36",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T06:29:58.230252Z",
     "iopub.status.busy": "2024-07-15T06:29:58.229486Z",
     "iopub.status.idle": "2024-07-15T06:31:18.893016Z",
     "shell.execute_reply": "2024-07-15T06:31:18.891995Z",
     "shell.execute_reply.started": "2024-07-15T06:29:58.230154Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Newtrain/data/sgns.sogou.char.bz2' has been successfully decompressed to 'Newtrain/data/sgns.sogou.char'.\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import os\n",
    "\n",
    "def decompress_bz2(file_path):\n",
    "    # 获取文件的基本路径和名称，用于构造解压后的文件名\n",
    "    base_path, filename = os.path.split(file_path)\n",
    "    output_file = os.path.join(base_path, filename[:-4])  # 移除'.bz2'扩展名\n",
    "    \n",
    "    with bz2.open(file_path, 'rb') as f_in:\n",
    "        with open(output_file, 'wb') as f_out:\n",
    "            data = f_in.read()\n",
    "            f_out.write(data)\n",
    "    \n",
    "    print(f\"File '{file_path}' has been successfully decompressed to '{output_file}'.\")\n",
    "\n",
    "# 指定你的.bz2文件路径\n",
    "bz2_file_path = 'Newtrain/data/sgns.sogou.char.bz2'\n",
    "\n",
    "# 调用函数解压文件\n",
    "decompress_bz2(bz2_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da137bde-11ea-4437-b0d3-0327779822d2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T07:55:21.693882Z",
     "iopub.status.busy": "2024-07-15T07:55:21.693258Z",
     "iopub.status.idle": "2024-07-15T07:55:21.706476Z",
     "shell.execute_reply": "2024-07-15T07:55:21.705496Z",
     "shell.execute_reply.started": "2024-07-15T07:55:21.693850Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.,  2.,  0.,  1.,  3.,  5.,  0.,  3.,  4.,  1.,  2.,  5.,  2.,\n",
       "          1.,  4.,  1.,  3.,  4., 10.,  2.],\n",
       "        [ 4.,  3.,  2.,  2.,  0.,  4.,  1.,  2.,  0.,  3.,  1.,  7.,  4.,\n",
       "          2.,  3.,  1.,  2.,  2.,  2.,  1.],\n",
       "        [ 2.,  2.,  3.,  0.,  0.,  0.,  0.,  6.,  0.,  3.,  4.,  0.,  5.,\n",
       "          1.,  0.,  0.,  1.,  2.,  4.,  0.],\n",
       "        [ 0.,  1.,  4.,  4.,  2.,  0.,  2.,  1.,  3.,  2.,  1.,  1.,  3.,\n",
       "          0.,  0.,  2.,  6.,  3.,  3.,  1.],\n",
       "        [ 0.,  0.,  7.,  2.,  1.,  0.,  1.,  2.,  1.,  2.,  2.,  1.,  4.,\n",
       "          0.,  5.,  5.,  0.,  0.,  4.,  2.],\n",
       "        [ 5.,  2.,  0.,  1.,  2.,  3.,  1.,  3.,  1.,  3.,  2.,  4.,  4.,\n",
       "          2.,  2.,  0.,  3.,  2.,  5., 10.],\n",
       "        [ 3.,  1.,  0.,  0.,  5.,  4.,  0.,  2.,  6.,  3.,  7.,  2.,  0.,\n",
       "          4.,  2.,  1.,  4.,  6.,  6.,  2.],\n",
       "        [ 5.,  2.,  1.,  4.,  3.,  3.,  0.,  3.,  0.,  7.,  2.,  2.,  1.,\n",
       "          2.,  2.,  2.,  0.,  0.,  4.,  0.],\n",
       "        [ 2.,  2.,  1.,  1.,  5.,  5.,  0.,  0.,  4.,  1.,  4.,  3.,  1.,\n",
       "          2.,  1.,  1.,  2.,  2.,  5.,  1.],\n",
       "        [ 1.,  2.,  2.,  5.,  3.,  3.,  3.,  1.,  3.,  3.,  4.,  2.,  1.,\n",
       "          0.,  2.,  3.,  1.,  6.,  3.,  2.]]),\n",
       " array([[0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "# 生成多标签数据集\n",
    "X, Y = make_multilabel_classification(n_samples=10, n_features=20, n_classes=10, random_state=42)\n",
    "X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b4ab42-3c8b-4ecb-aa04-ed2be05085ce",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T06:43:28.517899Z",
     "iopub.status.busy": "2024-07-15T06:43:28.517263Z",
     "iopub.status.idle": "2024-07-15T06:43:41.228464Z",
     "shell.execute_reply": "2024-07-15T06:43:41.227330Z",
     "shell.execute_reply.started": "2024-07-15T06:43:28.517865Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import build_dataset, build_iterator, get_time_dif, build_vocab\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "MAX_VOCAB_SIZE = 20000  # 词表长度限制\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "'''提取预训练词向量'''\n",
    "# 下面的目录、文件名按需更改。\n",
    "train_dir = \"Newtrain/data/train.txt\"\n",
    "vocab_dir = \"Newtrain/data/vocab.pkl\"\n",
    "pretrain_dir = \"Newtrain/data/sgns.sogou.char\"\n",
    "emb_dim = 300\n",
    "filename_trimmed_dir = \"Newtrain/data/embedding_Newtrain_\" + str(emb_dim)\n",
    "if os.path.exists(vocab_dir):\n",
    "    word_to_id = pkl.load(open(vocab_dir, 'rb'))\n",
    "else:\n",
    "    tokenizer = lambda x: x.split(' ')  # 以词为单位构建词表(数据集中词之间以空格隔开)\n",
    "    # tokenizer = lambda x: [y for y in x]  # 以字为单位构建词表\n",
    "    word_to_id = build_vocab(train_dir, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n",
    "    pkl.dump(word_to_id, open(vocab_dir, 'wb'))\n",
    "\n",
    "embeddings = np.random.rand(len(word_to_id), emb_dim)\n",
    "f = open(pretrain_dir, \"r\", encoding='UTF-8')\n",
    "for i, line in enumerate(f.readlines()):\n",
    "    # if i == 0:  # 若第一行是标题，则跳过\n",
    "    #     continue\n",
    "    lin = line.strip().split(\" \")\n",
    "    if lin[0] in word_to_id:\n",
    "        idx = word_to_id[lin[0]]\n",
    "        emb = [float(x) for x in lin[1:emb_dim+1]]\n",
    "        embeddings[idx] = np.asarray(emb, dtype='float32')\n",
    "f.close()\n",
    "np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6dfbb-fa0a-452e-bf32-fb2cf95ef8b4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d26c5-d39f-4094-883e-a0156c8788eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_(config, ues_word):\n",
    "    if ues_word:\n",
    "        tokenizer = lambda x: x.split(' ')  # 以空格隔开，word-level\n",
    "    else:\n",
    "        tokenizer = lambda x: [y for y in x]  # char-level\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n",
    "        pkl.dump(vocab, open(config.vocab_path, 'wb'))\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    def load_dataset(path, pad_size=32):\n",
    "        contents = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                content, label = lin.split('\\t')\n",
    "                words_line = []\n",
    "                token = tokenizer(content)\n",
    "                seq_len = len(token)\n",
    "                if pad_size:\n",
    "                    if len(token) < pad_size:\n",
    "                        token.extend([PAD] * (pad_size - len(token)))\n",
    "                    else:\n",
    "                        token = token[:pad_size]\n",
    "                        seq_len = pad_size\n",
    "                # word to id\n",
    "                for word in token:\n",
    "                    words_line.append(vocab.get(word, vocab.get(UNK)))\n",
    "                contents.append((words_line, int(label), seq_len))\n",
    "        return contents  # [([...], 0), ([...], 1), ...]\n",
    "    train = load_dataset(config.train_path, config.pad_size)\n",
    "    dev = load_dataset(config.dev_path, config.pad_size)\n",
    "    test = load_dataset(config.test_path, config.pad_size)\n",
    "    return vocab, train, dev, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340d1701-2b47-4556-bfd0-ff17280c19a5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T06:55:50.296569Z",
     "iopub.status.busy": "2024-07-15T06:55:50.295872Z",
     "iopub.status.idle": "2024-07-15T06:55:50.304067Z",
     "shell.execute_reply": "2024-07-15T06:55:50.302598Z",
     "shell.execute_reply.started": "2024-07-15T06:55:50.296534Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_list = [\"1\", \"2\", \"3\", \"4\"]\n",
    "int_list = list(map(int, str_list))\n",
    "int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd6db260-09b8-4cc9-b835-ade1ae59fa1d",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T07:15:09.856832Z",
     "iopub.status.busy": "2024-07-15T07:15:09.856130Z",
     "iopub.status.idle": "2024-07-15T07:15:10.273258Z",
     "shell.execute_reply": "2024-07-15T07:15:10.271819Z",
     "shell.execute_reply.started": "2024-07-15T07:15:09.856794Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:00, 19987.38it/s]\n"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "pad_size = 64\n",
    "from collections import defaultdict\n",
    "\n",
    "with open(train_dir, 'r', encoding='UTF-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        lin = line.strip()\n",
    "        if not lin:\n",
    "            continue\n",
    "        tmp_data = lin.split('\\t')\n",
    "        content = tmp_data[0]\n",
    "        zhuti_label = [t.split('#')[0] for t in tmp_data[1:]]\n",
    "\n",
    "        cal_qinggan = sum([int(t.split('#')[1]) for t in tmp_data[1:]])\n",
    "        qinggan_label = 0\n",
    "        if cal_qinggan > 0:\n",
    "            qinggan_label = 1\n",
    "        elif cal_qinggan < 0:\n",
    "            qinggan_label = -1\n",
    "\n",
    "        words_line = []\n",
    "        token = tokenizer(content)\n",
    "        seq_len = len(token)\n",
    "        if pad_size:\n",
    "            if len(token) < pad_size:\n",
    "                token.extend([PAD] * (pad_size - len(token)))\n",
    "            else:\n",
    "                token = token[:pad_size]\n",
    "                seq_len = pad_size\n",
    "        # word to id\n",
    "        for word in token:\n",
    "            words_line.append(word_to_id.get(word, word_to_id.get(UNK)))\n",
    "        contents.append((words_line, zhuti_label, int(qinggan_label), seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71c8f604-5358-469e-99bf-c0081ac9a594",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T07:15:13.797016Z",
     "iopub.status.busy": "2024-07-15T07:15:13.796490Z",
     "iopub.status.idle": "2024-07-15T07:15:13.805606Z",
     "shell.execute_reply": "2024-07-15T07:15:13.803309Z",
     "shell.execute_reply.started": "2024-07-15T07:15:13.796962Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int,\n",
       "             {'价格': 1237,\n",
       "              '配置': 812,\n",
       "              '操控': 986,\n",
       "              '舒适性': 916,\n",
       "              '油耗': 1043,\n",
       "              '动力': 2643,\n",
       "              '内饰': 515,\n",
       "              '安全性': 537,\n",
       "              '空间': 433,\n",
       "              '外观': 479}),\n",
       " defaultdict(int, {0: 5660, -1: 1196, 1: 1144}))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cc78879-a479-4b2f-a23b-64541022fa36",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T07:18:19.150374Z",
     "iopub.status.busy": "2024-07-15T07:18:19.149679Z",
     "iopub.status.idle": "2024-07-15T07:18:19.156828Z",
     "shell.execute_reply": "2024-07-15T07:18:19.155644Z",
     "shell.execute_reply.started": "2024-07-15T07:18:19.150338Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['价格', '配置', '操控', '舒适性', '油耗', '动力', '内饰', '安全性', '空间', '外观'])\n"
     ]
    }
   ],
   "source": [
    "print(k.keys())\n",
    "class_name = ['价格', '配置', '操控', '舒适性', '油耗', '动力', '内饰', '安全性', '空间', '外观']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "305dc69c-3c9b-407e-95f2-fdedf2865866",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T09:16:26.656386Z",
     "iopub.status.busy": "2024-07-15T09:16:26.655683Z",
     "iopub.status.idle": "2024-07-15T09:16:26.662759Z",
     "shell.execute_reply": "2024-07-15T09:16:26.661711Z",
     "shell.execute_reply.started": "2024-07-15T09:16:26.656350Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = []\n",
    "for tmp in [[1,5], [2, 3], [0], [2]]:\n",
    "    z.append(torch.sum(F.one_hot(torch.tensor(tmp), 10), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96cf2b3c-2175-4538-85d6-78d6af71a865",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T09:16:27.462598Z",
     "iopub.status.busy": "2024-07-15T09:16:27.461875Z",
     "iopub.status.idle": "2024-07-15T09:16:27.470506Z",
     "shell.execute_reply": "2024-07-15T09:16:27.469236Z",
     "shell.execute_reply.started": "2024-07-15T09:16:27.462547Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c29fb883-fc18-48b6-b8b4-1bcfddddd1b6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T08:47:13.872469Z",
     "iopub.status.busy": "2024-07-15T08:47:13.871801Z",
     "iopub.status.idle": "2024-07-15T08:47:13.890782Z",
     "shell.execute_reply": "2024-07-15T08:47:13.889513Z",
     "shell.execute_reply.started": "2024-07-15T08:47:13.872434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_470/2538150912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 假设labels现在是类别索引，比如[0, 1, 2]表示第0、1、2个标签存在\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlabels_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 示例索引表示的标签\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# 假设有3个可能的标签类别\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 假设labels现在是类别索引，比如[0, 1, 2]表示第0、1、2个标签存在\n",
    "labels_indices = torch.tensor([[0, 5], [1, 2], [0]], dtype=torch.long)  # 示例索引表示的标签\n",
    "num_classes = 10 # 假设有3个可能的标签类别\n",
    "\n",
    "# 转换为one-hot编码\n",
    "one_hot_labels = F.one_hot(labels_indices, num_classes)\n",
    "print(one_hot_labels)\n",
    "torch.sum(one_hot_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4f39f7d-6cd3-451a-b36d-cde77f19e682",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-15T08:31:53.555386Z",
     "iopub.status.busy": "2024-07-15T08:31:53.554700Z",
     "iopub.status.idle": "2024-07-15T08:31:53.567382Z",
     "shell.execute_reply": "2024-07-15T08:31:53.566026Z",
     "shell.execute_reply.started": "2024-07-15T08:31:53.555349Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1., 0., 1.]])\n",
      "tensor([[-3.5894e-01,  4.7935e-01, -2.3828e-01, -1.3542e+00,  2.6869e-01,\n",
      "          1.1456e-01, -1.5563e+00, -1.0757e+00, -5.7266e-02,  2.2000e+00],\n",
      "        [ 9.9124e-01, -5.8622e-02,  1.1788e+00,  6.2218e-01,  7.8785e-01,\n",
      "          1.3686e+00, -1.2211e+00, -6.7527e-01,  1.0476e+00, -3.1758e-01],\n",
      "        [ 1.3949e-01,  2.3403e+00, -6.1161e-01,  8.1603e-01,  6.7713e-01,\n",
      "          6.8086e-01,  1.9948e-01,  7.9927e-01, -2.6190e-01,  1.5133e-01],\n",
      "        [ 1.1982e+00, -2.2833e+00,  3.3981e-01, -1.1494e+00, -6.0679e-01,\n",
      "         -5.2522e-01, -5.6619e-01,  6.6040e-04,  7.2246e-01,  1.5264e-01],\n",
      "        [ 1.4496e-01, -2.3442e+00,  3.6000e-01,  4.6668e-01,  1.2831e+00,\n",
      "          1.2678e+00,  1.9883e-01,  5.4409e-01, -3.9782e-01, -1.9291e+00]])\n",
      "Loss: 0.8391042947769165\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 假设我们有3个类别，每个样本可以属于这些类别的任意子集\n",
    "num_classes = 10\n",
    "batch_size = 3\n",
    "\n",
    "# 随机生成一些数据和目标标签（假设已经进行了适当的预处理）\n",
    "inputs = torch.randn(batch_size, num_classes)  # 模型的输入logits\n",
    "targets = torch.bernoulli(torch.rand(batch_size, num_classes))  # 生成0或1的目标标签，模拟多标签情况\n",
    "print(targets)\n",
    "# 将数据包装为Variable，如果使用的是PyTorch的较新版本，这一步通常不需要\n",
    "inputs = Variable(inputs)\n",
    "targets = Variable(targets)\n",
    "\n",
    "# 定义模型，这里简化处理，直接使用inputs作为模型输出\n",
    "model_output = inputs\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(model_output)\n",
    "# 计算损失\n",
    "loss = criterion(model_output, targets)\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
