{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f23745-83cc-4b8e-9176-5116368e3316",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-16T01:40:37.862314Z",
     "iopub.status.busy": "2024-07-16T01:40:37.862053Z",
     "iopub.status.idle": "2024-07-16T01:40:39.031038Z",
     "shell.execute_reply": "2024-07-16T01:40:39.030478Z",
     "shell.execute_reply.started": "2024-07-16T01:40:37.862292Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 12678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:00, 69866.49it/s]\n",
      "2653it [00:00, 70938.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from train_eval import train, init_network\n",
    "from importlib import import_module\n",
    "import argparse\n",
    "dataset = 'Newtrain_qinggan'  # 数据集\n",
    "\n",
    "# 搜狗新闻:embedding_SougouNews.npz, 腾讯:embedding_Tencent.npz, 随机初始化:random\n",
    "embedding = 'embedding_Newtrain_300.npz'\n",
    "\n",
    "model_name = 'TextRCNN'  # 'TextRCNN'  # TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer\n",
    "\n",
    "\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config(dataset, embedding)\n",
    "config.pad_size = 64\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000  # 词表长度限制\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "ues_word = True\n",
    "from utils import get_time_dif, build_vocab, build_dataset2, build_iterator2\n",
    "\n",
    "\n",
    "vocab, train, test = build_dataset2(config, ues_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f8cba0-8f6a-408f-9f32-368cddce6ce7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-16T01:40:40.855092Z",
     "iopub.status.busy": "2024-07-16T01:40:40.854757Z",
     "iopub.status.idle": "2024-07-16T01:40:40.858166Z",
     "shell.execute_reply": "2024-07-16T01:40:40.857685Z",
     "shell.execute_reply.started": "2024-07-16T01:40:40.855072Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "train_iter = build_iterator2(train, config)\n",
    "test_iter = build_iterator2(test, config)\n",
    "# train_iter.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e566d-dc8c-46f1-a15e-696b712a85c7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.n_vocab = len(vocab)\n",
    "model = x.Model(config).to(config.device)\n",
    "if model_name != 'Transformer':\n",
    "    init_network(model)\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44af53-8aad-4125-8e14-7eab47624774",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-16T01:40:44.805311Z",
     "iopub.status.busy": "2024-07-16T01:40:44.804792Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Iter:      0,  Train Loss:  0.99,  Train Acc: 58.59%,  Val Loss:   0.9,  Val Acc: 68.68%,  Time: 0:05:39 *improve\n",
      "Iter:      2,  Train Loss:   1.0,  Train Acc: 56.25%,  Val Loss:  0.89,  Val Acc: 68.68%,  Time: 0:11:37 *improve\n",
      "Iter:      4,  Train Loss:  0.79,  Train Acc: 73.44%,  Val Loss:  0.87,  Val Acc: 68.68%,  Time: 0:12:05 *improve\n",
      "Iter:      6,  Train Loss:  0.46,  Train Acc: 91.41%,  Val Loss:   1.0,  Val Acc: 68.68%,  Time: 0:12:30 \n",
      "Iter:      8,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:   1.3,  Val Acc: 68.68%,  Time: 0:12:55 \n",
      "Iter:     10,  Train Loss:   2.2,  Train Acc: 42.97%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:13:20 \n",
      "Iter:     12,  Train Loss:   1.2,  Train Acc: 60.16%,  Val Loss:   0.9,  Val Acc: 68.68%,  Time: 0:13:46 \n",
      "Iter:     14,  Train Loss:   1.1,  Train Acc: 53.91%,  Val Loss:  0.91,  Val Acc: 68.15%,  Time: 0:14:10 \n",
      "Iter:     16,  Train Loss:  0.98,  Train Acc: 63.28%,  Val Loss:   1.0,  Val Acc: 53.60%,  Time: 0:14:37 \n",
      "Iter:     18,  Train Loss:   1.1,  Train Acc: 39.06%,  Val Loss:   1.1,  Val Acc: 37.09%,  Time: 0:15:28 \n",
      "Iter:     20,  Train Loss:   1.0,  Train Acc: 39.84%,  Val Loss:  0.98,  Val Acc: 63.74%,  Time: 0:15:55 \n",
      "Iter:     22,  Train Loss:  0.77,  Train Acc: 85.94%,  Val Loss:  0.88,  Val Acc: 68.64%,  Time: 0:16:21 \n",
      "Iter:     24,  Train Loss:  0.48,  Train Acc: 93.75%,  Val Loss:  0.88,  Val Acc: 68.68%,  Time: 0:16:48 \n",
      "Iter:     26,  Train Loss:  0.42,  Train Acc: 89.84%,  Val Loss:  0.99,  Val Acc: 68.68%,  Time: 0:17:16 \n",
      "Iter:     28,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:18:08 \n",
      "Iter:     30,  Train Loss:  0.87,  Train Acc: 77.34%,  Val Loss:   1.2,  Val Acc: 68.68%,  Time: 0:18:33 \n",
      "Iter:     32,  Train Loss:  0.85,  Train Acc: 78.91%,  Val Loss:   1.3,  Val Acc: 68.68%,  Time: 0:18:57 \n",
      "Iter:     34,  Train Loss:   1.0,  Train Acc: 72.66%,  Val Loss:   1.2,  Val Acc: 68.68%,  Time: 0:19:22 \n",
      "Iter:     36,  Train Loss:  0.36,  Train Acc: 91.41%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:20:17 \n",
      "Iter:     38,  Train Loss:  0.27,  Train Acc: 93.75%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:20:41 \n",
      "Iter:     40,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:   1.0,  Val Acc: 68.68%,  Time: 0:21:05 \n",
      "Iter:     42,  Train Loss:  0.55,  Train Acc: 84.38%,  Val Loss:   1.0,  Val Acc: 68.68%,  Time: 0:21:28 \n",
      "Iter:     44,  Train Loss:   1.2,  Train Acc: 60.16%,  Val Loss:  0.97,  Val Acc: 68.68%,  Time: 0:21:53 \n",
      "Iter:     46,  Train Loss:   1.0,  Train Acc: 63.28%,  Val Loss:  0.88,  Val Acc: 68.68%,  Time: 0:22:17 \n",
      "Iter:     48,  Train Loss:   1.0,  Train Acc: 58.59%,  Val Loss:  0.84,  Val Acc: 68.68%,  Time: 0:22:44 *improve\n",
      "Iter:     50,  Train Loss:  0.72,  Train Acc: 75.78%,  Val Loss:  0.85,  Val Acc: 68.68%,  Time: 0:23:10 \n",
      "Iter:     52,  Train Loss:  0.87,  Train Acc: 67.97%,  Val Loss:  0.88,  Val Acc: 68.68%,  Time: 0:23:36 \n",
      "Iter:     54,  Train Loss:  0.92,  Train Acc: 62.50%,  Val Loss:  0.88,  Val Acc: 68.71%,  Time: 0:24:02 \n",
      "Iter:     56,  Train Loss:   1.1,  Train Acc: 41.41%,  Val Loss:  0.87,  Val Acc: 68.71%,  Time: 0:24:27 \n",
      "Iter:     58,  Train Loss:   1.1,  Train Acc: 49.22%,  Val Loss:  0.87,  Val Acc: 68.75%,  Time: 0:24:55 \n",
      "Iter:     60,  Train Loss:  0.98,  Train Acc: 53.91%,  Val Loss:  0.87,  Val Acc: 68.75%,  Time: 0:25:24 \n",
      "Iter:     62,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 68.56%,  Time: 0:25:52 \n",
      "Epoch [2/10]\n",
      "Iter:     64,  Train Loss:   0.9,  Train Acc: 67.97%,  Val Loss:  0.93,  Val Acc: 68.15%,  Time: 0:26:21 \n",
      "Iter:     66,  Train Loss:  0.87,  Train Acc: 71.88%,  Val Loss:   0.9,  Val Acc: 68.34%,  Time: 0:26:48 \n",
      "Iter:     68,  Train Loss:  0.73,  Train Acc: 83.59%,  Val Loss:  0.85,  Val Acc: 68.64%,  Time: 0:27:14 \n",
      "Iter:     70,  Train Loss:  0.54,  Train Acc: 89.06%,  Val Loss:  0.83,  Val Acc: 68.68%,  Time: 0:27:44 *improve\n",
      "Iter:     72,  Train Loss:   1.3,  Train Acc: 41.41%,  Val Loss:  0.88,  Val Acc: 68.68%,  Time: 0:28:08 \n",
      "Iter:     74,  Train Loss:   1.3,  Train Acc: 50.00%,  Val Loss:  0.89,  Val Acc: 68.68%,  Time: 0:28:33 \n",
      "Iter:     76,  Train Loss:   1.4,  Train Acc: 38.28%,  Val Loss:  0.86,  Val Acc: 68.68%,  Time: 0:28:58 \n",
      "Iter:     78,  Train Loss:  0.82,  Train Acc: 67.19%,  Val Loss:  0.82,  Val Acc: 68.68%,  Time: 0:29:23 *improve\n",
      "Iter:     80,  Train Loss:  0.89,  Train Acc: 59.38%,  Val Loss:  0.82,  Val Acc: 68.83%,  Time: 0:29:51 *improve\n",
      "Iter:     82,  Train Loss:  0.83,  Train Acc: 65.62%,  Val Loss:  0.85,  Val Acc: 68.07%,  Time: 0:30:15 \n",
      "Iter:     84,  Train Loss:  0.84,  Train Acc: 66.41%,  Val Loss:  0.86,  Val Acc: 67.77%,  Time: 0:30:38 \n",
      "Iter:     86,  Train Loss:  0.73,  Train Acc: 82.81%,  Val Loss:  0.84,  Val Acc: 68.11%,  Time: 0:31:02 \n",
      "Iter:     88,  Train Loss:  0.57,  Train Acc: 89.06%,  Val Loss:  0.81,  Val Acc: 68.79%,  Time: 0:31:28 *improve\n",
      "Iter:     90,  Train Loss:  0.49,  Train Acc: 88.28%,  Val Loss:  0.84,  Val Acc: 68.68%,  Time: 0:31:56 \n",
      "Iter:     92,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.93,  Val Acc: 68.68%,  Time: 0:32:24 \n",
      "Iter:     94,  Train Loss:  0.45,  Train Acc: 86.72%,  Val Loss:   1.0,  Val Acc: 68.68%,  Time: 0:37:30 \n",
      "Iter:     96,  Train Loss:  0.81,  Train Acc: 74.22%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:46:45 \n",
      "Iter:     98,  Train Loss:  0.49,  Train Acc: 85.94%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:54:54 \n",
      "Iter:    100,  Train Loss:  0.13,  Train Acc: 98.44%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:58:20 \n",
      "Iter:    102,  Train Loss:  0.51,  Train Acc: 85.16%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:59:14 \n",
      "Iter:    104,  Train Loss:  0.31,  Train Acc: 92.19%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 0:59:43 \n",
      "Iter:    106,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:   1.1,  Val Acc: 68.68%,  Time: 1:05:03 \n",
      "Iter:    108,  Train Loss:   1.2,  Train Acc: 58.59%,  Val Loss:  0.98,  Val Acc: 68.68%,  Time: 1:07:41 \n",
      "Iter:    110,  Train Loss:  0.82,  Train Acc: 70.31%,  Val Loss:  0.87,  Val Acc: 68.68%,  Time: 1:08:10 \n",
      "Iter:    112,  Train Loss:   1.1,  Train Acc: 50.00%,  Val Loss:   0.8,  Val Acc: 68.75%,  Time: 1:08:36 *improve\n",
      "Iter:    114,  Train Loss:   0.7,  Train Acc: 75.00%,  Val Loss:   0.8,  Val Acc: 68.07%,  Time: 1:09:01 \n",
      "Iter:    116,  Train Loss:   0.6,  Train Acc: 95.31%,  Val Loss:  0.84,  Val Acc: 66.60%,  Time: 1:09:47 \n",
      "Iter:    118,  Train Loss:  0.89,  Train Acc: 57.81%,  Val Loss:  0.86,  Val Acc: 64.98%,  Time: 1:10:51 \n",
      "Iter:    120,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.87,  Val Acc: 64.12%,  Time: 1:14:38 \n",
      "Iter:    122,  Train Loss:  0.97,  Train Acc: 50.00%,  Val Loss:  0.88,  Val Acc: 64.00%,  Time: 1:17:27 \n",
      "Iter:    124,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:   0.9,  Val Acc: 63.93%,  Time: 1:17:55 \n",
      "Epoch [3/10]\n",
      "Iter:    126,  Train Loss:  0.83,  Train Acc: 70.31%,  Val Loss:  0.92,  Val Acc: 61.33%,  Time: 1:18:22 \n",
      "Iter:    128,  Train Loss:  0.93,  Train Acc: 60.16%,  Val Loss:  0.88,  Val Acc: 64.23%,  Time: 1:18:53 \n",
      "Iter:    130,  Train Loss:  0.68,  Train Acc: 78.91%,  Val Loss:  0.81,  Val Acc: 68.11%,  Time: 1:19:19 \n",
      "Iter:    132,  Train Loss:  0.49,  Train Acc: 90.62%,  Val Loss:  0.78,  Val Acc: 68.22%,  Time: 1:19:47 *improve\n",
      "Iter:    134,  Train Loss:  0.36,  Train Acc: 92.19%,  Val Loss:  0.84,  Val Acc: 68.75%,  Time: 1:20:13 \n",
      "Iter:    136,  Train Loss:   1.2,  Train Acc: 46.09%,  Val Loss:  0.89,  Val Acc: 68.68%,  Time: 1:20:36 \n",
      "Iter:    138,  Train Loss:  0.95,  Train Acc: 60.94%,  Val Loss:  0.86,  Val Acc: 68.83%,  Time: 1:21:01 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from utils import get_time_dif\n",
    "from tensorboardX import SummaryWriter\n",
    "def train_qinggan(config, model, train_iter, dev_iter):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, qinggan_label, zhuti_label) in enumerate(train_iter):\n",
    "            print('Batch: ', i)\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, qinggan_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 2 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = qinggan_label.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*improve'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    writer.close()\n",
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, qinggan_label, zhuti_label in data_iter:\n",
    "            outputs = model(texts)\n",
    "            loss = F.cross_entropy(outputs, qinggan_label)\n",
    "            loss_total += loss\n",
    "            labels = qinggan_label.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)\n",
    "train_qinggan(config, model, train_iter, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be3655-697f-48da-8da8-4acfd530b84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054ef228-bb69-47e0-a29a-2ac92cfd171c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-16T03:21:27.072676Z",
     "iopub.status.busy": "2024-07-16T03:21:27.071834Z",
     "iopub.status.idle": "2024-07-16T03:21:27.170645Z",
     "shell.execute_reply": "2024-07-16T03:21:27.081559Z",
     "shell.execute_reply.started": "2024-07-16T03:21:27.072641Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0402, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    参考 https://github.com/lonePatient/TorchBlocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=1, epsilon=1.e-9, device=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha, device=device)\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: model's output, shape of [batch_size, num_cls]\n",
    "            target: ground truth labels, shape of [batch_size]\n",
    "        Returns:\n",
    "            shape of [batch_size]\n",
    "        \"\"\"\n",
    "        num_labels = input.size(-1)\n",
    "        idx = target.view(-1, 1).long()\n",
    "        one_hot_key = torch.zeros(idx.size(0), num_labels, dtype=torch.float32, device=idx.device)\n",
    "        one_hot_key = one_hot_key.scatter_(1, idx, 1)\n",
    "        one_hot_key[:, 0] = 0  # ignore 0 index.\n",
    "        logits = torch.softmax(input, dim=-1)\n",
    "        loss = -self.alpha * one_hot_key * torch.pow((1 - logits), self.gamma) * (logits + self.epsilon).log()\n",
    "        loss = loss.sum(1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "loss = FocalLoss(alpha=[0.1, 0.2, 0.3])\n",
    "input = torch.randn(5, 3, requires_grad=True)\n",
    "target = torch.empty(5, dtype=torch.long).random_(3)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1cee9-7d1c-4b2c-a8fc-877f8f80a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters,lr=config.learning_rate,eps=1e-8)\n",
    "\n",
    "# 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = len(train_iter) * config.num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ab434-3d95-43e4-9a46-23dcfefd2ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
